{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# ü¶∑ NHANES Periodontitis Prediction: Modern Gradient Boosting Benchmark\n",
    "\n",
    "**Author:** Francisco Teixeira Barbosa (Cisco @ Periospot)  \n",
    "**Date:** November 2025  \n",
    "**Project:** Systematic Comparison of XGBoost, CatBoost, and LightGBM for Periodontitis Prediction\n",
    "\n",
    "---\n",
    "\n",
    "## üìÑ Reference Paper\n",
    "\n",
    "**Bashir NZ, Gill S, Tawse-Smith A, Torkzaban P, Graf D, Gary MT.**  \n",
    "*Systematic comparison of machine learning algorithms to develop and validate predictive models for periodontitis.*  \n",
    "**J Clin Periodontol.** 2022;49:958-969.\n",
    "\n",
    "üìÅ **Paper Location:** `scientific_articles/J Clinic Periodontology - 2022 - Bashir...pdf`\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Project Goals & Rationale\n",
    "\n",
    "### The Problem\n",
    "**Periodontitis** affects ~68% of US adults aged 30+ (per our NHANES 2011-2014 analysis), yet early prediction remains challenging.\n",
    "\n",
    "**Bashir et al. (2022)** tested 10 ML algorithms and achieved excellent internal validation (AUC > 0.95), but they **did NOT evaluate modern gradient boosting methods** (XGBoost, CatBoost, LightGBM).\n",
    "\n",
    "### Key Research Gap\n",
    "\n",
    "From **Polizzi et al. (2024)** systematic review:  \n",
    "> \"None of the included articles used more powerful networks [referring to modern gradient boosting methods]\"\n",
    "\n",
    "**This study fills that gap** by being the **first** to systematically compare XGBoost, CatBoost, and LightGBM for periodontitis prediction.\n",
    "\n",
    "### Our Approach: Cross-Validation with Modern Methods\n",
    "\n",
    "**Dataset:** 9,379 participants from NHANES 2011-2014 with full periodontal measurements\n",
    "\n",
    "**Validation Strategy:** Stratified 5-fold cross-validation\n",
    "- ‚úÖ Robust performance estimates with 95% confidence intervals\n",
    "- ‚úÖ Full use of available data\n",
    "- ‚úÖ Fair comparison to Bashir's internal validation approach\n",
    "\n",
    "**Why only 2011-2014?**\n",
    "‚ö†Ô∏è **Critical Data Limitation:** NHANES discontinued full-mouth periodontal examinations after 2013-2014. The 2015-2018 cycles lack the pocket depth (PD) and clinical attachment loss (CAL) measurements required for CDC/AAP classification.\n",
    "\n",
    "### Methodological Improvements Over Bashir\n",
    "\n",
    "1. **Modern Gradient Boosting:** XGBoost, CatBoost, LightGBM (NOT tested by Bashir)\n",
    "2. **Advanced Hyperparameter Optimization:** Optuna Bayesian search (vs. grid search)\n",
    "3. **Calibration:** Isotonic regression for well-calibrated probability predictions\n",
    "4. **Interpretability:** SHAP analysis for clinical trust\n",
    "5. **Survey Weights:** Sensitivity analysis with NHANES sampling weights\n",
    "6. **Full Reproducibility:** Open code, versioned artifacts, documented decisions\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Success Metrics & Hypotheses\n",
    "\n",
    "| Metric | Bashir Baselines | **Our Target** (XGBoost/CatBoost/LightGBM) |\n",
    "|--------|------------------|-------------------------------------------|\n",
    "| **AUC-ROC** | 0.95+ | **0.90‚Äì0.97** (match or exceed) |\n",
    "| **PR-AUC** | Not reported | **0.85‚Äì0.92** |\n",
    "| **Calibration (Brier)** | Not reported | **< 0.15** (well-calibrated) |\n",
    "| **F1-Score** | Not reported | **0.75‚Äì0.85** |\n",
    "\n",
    "**Primary Hypothesis:** Modern gradient boosting methods will achieve **comparable or better** performance than Bashir's best models (Random Forest, SVM, ANN) while providing:\n",
    "- ‚úÖ Better calibrated probabilities\n",
    "- ‚úÖ Clinical interpretability via SHAP\n",
    "- ‚úÖ Faster training times\n",
    "- ‚úÖ Better handling of missing data\n",
    "\n",
    "**Success Criteria:**\n",
    "1. At least one gradient boosting method exceeds Bashir's best baseline\n",
    "2. SHAP analysis reveals clinically interpretable risk factors\n",
    "3. Well-calibrated probability predictions (Brier score < 0.15)\n",
    "4. Reproducible results across 5 cross-validation folds\n",
    "\n",
    "---\n",
    "\n",
    "## üó∫Ô∏è Notebook Roadmap\n",
    "\n",
    "This notebook has **18 sections** organized into **5 phases**:\n",
    "\n",
    "### Phase 1: Data Acquisition & Labeling ‚úÖ (Sections 1‚Äì5)\n",
    "1. Environment setup\n",
    "2. Load configuration\n",
    "3. Download NHANES data (2011-2014)\n",
    "4. Merge components\n",
    "5. Apply CDC/AAP case definitions\n",
    "\n",
    "### Phase 2: Feature Engineering & EDA (Sections 6‚Äì7)\n",
    "6. Build 15 Bashir predictors\n",
    "7. Exploratory analysis & class balance\n",
    "\n",
    "### Phase 3: Baseline Models with Cross-Validation (Sections 8‚Äì10)\n",
    "8. Setup 5-fold stratified cross-validation\n",
    "9. Preprocessing pipelines (imputation + scaling)\n",
    "10. Baseline models (Logistic Regression, Random Forest)\n",
    "\n",
    "### Phase 4: Gradient Boosting with Optuna (Sections 11‚Äì13)\n",
    "11. XGBoost with Bayesian hyperparameter optimization\n",
    "12. CatBoost with Bayesian hyperparameter optimization\n",
    "13. LightGBM with Bayesian hyperparameter optimization\n",
    "\n",
    "### Phase 5: Interpretation & Export (Sections 14‚Äì18)\n",
    "14. Model comparison & statistical testing\n",
    "15. Calibration curves & isotonic regression\n",
    "16. SHAP feature importance analysis\n",
    "17. Decision curve analysis\n",
    "18. Save artifacts, model cards, & reproducibility log\n",
    "\n",
    "**Key Change from Original Plan:**  \n",
    "‚ö†Ô∏è Originally planned temporal validation (train 2011-2014, validate 2015-2016, test 2017-2018), but NHANES discontinued periodontal exams after 2013-2014. Pivoted to stratified 5-fold cross-validation, which is more appropriate given data constraints.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Important Notes Before Starting\n",
    "\n",
    "1. **Read the Config First:** All parameters are in `configs/config.yaml`\n",
    "2. **Data Limitation Acknowledged:** Only 2011-2014 cycles have full periodontal data (9,379 participants)\n",
    "3. **Cross-Validation Strategy:** Using stratified 5-fold CV instead of temporal split\n",
    "4. **Implement Sequentially:** Each section builds on previous ones\n",
    "5. **Test as You Go:** Run cells immediately to catch errors early\n",
    "6. **CDC/AAP Classification:** Already completed (Section 5) - 68% prevalence confirmed\n",
    "7. **Survey Weights:** For ML training, we use unweighted data; report weighted prevalence for publication\n",
    "8. **Reproducibility:** Random seed = 42 throughout; all results should be reproducible\n",
    "\n",
    "---\n",
    "\n",
    "Let's begin! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Working directory set to: /Users/franciscoteixeirabarbosa/Dropbox/Random_scripts/nhanes_periodontitis_ml\n",
      "‚úÖ Periospot color palette loaded:\n",
      "   periospot_blue: #15365a\n",
      "   mystic_blue: #003049\n",
      "   periospot_red: #6c1410\n",
      "   crimson_blaze: #a92a2a\n",
      "   vanilla_cream: #f7f0da\n",
      "   black: #000000\n",
      "   white: #ffffff\n",
      "\n",
      "üì¶ Package Versions:\n",
      "   pandas: 2.3.2\n",
      "   numpy: 2.3.5\n",
      "   scikit-learn: 1.7.1\n",
      "   xgboost: 3.1.1\n",
      "   catboost: 1.2.8\n",
      "   lightgbm: 4.6.0\n",
      "   optuna: 4.6.0\n",
      "   shap: 0.50.0\n",
      "‚úÖ Section 1 Complete: Environment configured, seed set, Periospot style applied\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Section 1: Environment Setup & Imports\n",
    "========================================\n",
    "Set up the computational environment with all required libraries,\n",
    "apply reproducibility measures, and configure Periospot plotting style.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def find_project_root(marker: str = \"configs/config.yaml\") -> Path:\n",
    "    \"\"\"Find project root by searching upward for a marker file.\"\"\"\n",
    "    here = Path.cwd().resolve()\n",
    "    for candidate in [here] + list(here.parents):\n",
    "        if (candidate / marker).exists():\n",
    "            return candidate\n",
    "    raise FileNotFoundError(f\"Could not locate {marker} from {here}\")\n",
    "\n",
    "# Set working directory to project root\n",
    "BASE_DIR = find_project_root()\n",
    "os.chdir(BASE_DIR)\n",
    "print(f\"‚úÖ Working directory set to: {Path.cwd()}\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, brier_score_loss,\n",
    "    accuracy_score, recall_score, precision_score, f1_score,\n",
    "    confusion_matrix, roc_curve, precision_recall_curve\n",
    ")\n",
    "\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import lightgbm as lgb\n",
    "\n",
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "import shap\n",
    "\n",
    "import yaml\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, str(Path.cwd() / 'src'))\n",
    "\n",
    "from ps_plot import set_style, get_palette, save_figure\n",
    "from labels import label_periodontitis\n",
    "from evaluation import compute_metrics, plot_roc_pr, select_threshold, plot_calibration_curve\n",
    "from utils import set_seed, save_json, log_versions, save_model\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "set_seed(RANDOM_SEED)\n",
    "\n",
    "set_style()\n",
    "palette = get_palette()\n",
    "print(\"‚úÖ Periospot color palette loaded:\")\n",
    "for name, hex_code in palette.items():\n",
    "    print(f\"   {name}: {hex_code}\")\n",
    "\n",
    "print(\"\\nüì¶ Package Versions:\")\n",
    "print(f\"   pandas: {pd.__version__}\")\n",
    "print(f\"   numpy: {np.__version__}\")\n",
    "print(f\"   scikit-learn: {sklearn.__version__}\")\n",
    "print(f\"   xgboost: {xgb.__version__}\")\n",
    "print(f\"   catboost: {cb.__version__}\")\n",
    "print(f\"   lightgbm: {lgb.__version__}\")\n",
    "print(f\"   optuna: {optuna.__version__}\")\n",
    "print(f\"   shap: {shap.__version__}\")\n",
    "\n",
    "print(\"‚úÖ Section 1 Complete: Environment configured, seed set, Periospot style applied\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ded3e4",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Load Configuration\n",
    "\n",
    "**Load:** `configs/config.yaml`\n",
    "\n",
    "**Contains:** NHANES cycles (2011-2014), validation strategy (5-fold CV), 15 predictors, CDC/AAP definitions, Optuna params, Periospot colors, survey weights\n",
    "\n",
    "**Note:** Only 2011-2014 cycles have full periodontal measurements. 2015-2018 cycles were excluded due to missing PD/CAL data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebb7e15",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'temporal_split'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m [RAW_DIR, PROCESSED_DIR, FIGURES_DIR, MODELS_DIR, RESULTS_DIR, ARTIFACTS_DIR, LOGS_DIR]:\n\u001b[32m     22\u001b[39m     d.mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain cycles: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtemporal_split\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mValidation cycles: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[33m'\u001b[39m\u001b[33mtemporal_split\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mvalidation\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTest cycles: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[33m'\u001b[39m\u001b[33mtemporal_split\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: 'temporal_split'"
     ]
    }
   ],
   "source": [
    "# Load config.yaml and derive cycles/components/paths\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "CONFIG_PATH = Path.cwd() / \"configs\" / \"config.yaml\"\n",
    "with open(CONFIG_PATH) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "CYCLES = config[\"cycles\"][\"all\"]\n",
    "CYCLE_SUFFIX = config[\"cycle_suffixes\"]\n",
    "COMPONENTS = config[\"components\"]\n",
    "BASE_URL = config[\"base_url\"]\n",
    "\n",
    "RAW_DIR = Path.cwd() / config[\"paths\"][\"data_raw\"]\n",
    "PROCESSED_DIR = Path.cwd() / config[\"paths\"][\"data_processed\"]\n",
    "FIGURES_DIR = Path.cwd() / config[\"paths\"][\"figures\"]\n",
    "MODELS_DIR = Path.cwd() / config[\"paths\"][\"models\"]\n",
    "RESULTS_DIR = Path.cwd() / config[\"paths\"][\"results\"]\n",
    "ARTIFACTS_DIR = Path.cwd() / config[\"paths\"][\"artifacts\"]\n",
    "LOGS_DIR = Path.cwd() / config[\"paths\"][\"logs\"]\n",
    "for d in [RAW_DIR, PROCESSED_DIR, FIGURES_DIR, MODELS_DIR, RESULTS_DIR, ARTIFACTS_DIR, LOGS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üìä Dataset Configuration:\")\n",
    "print(f\"   Cycles: {CYCLES}\")\n",
    "print(f\"   Total cycles: {len(CYCLES)}\")\n",
    "print(f\"   Validation strategy: {config['validation_strategy']['method']}\")\n",
    "print(f\"   Number of folds: {config['validation_strategy']['n_folds']}\")\n",
    "print(f\"   Random state: {config['validation_strategy']['random_state']}\")\n",
    "print(f\"\\nüìÅ Data directories:\")\n",
    "print(f\"   Raw data: {RAW_DIR}\")\n",
    "print(f\"   Processed data: {PROCESSED_DIR}\")\n",
    "print(f\"   Figures: {FIGURES_DIR}\")\n",
    "print(f\"   Models: {MODELS_DIR}\")\n",
    "print(\"\\n‚úÖ Section 2: Config loaded (using 2011-2014 cycles with 5-fold CV)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aafdaf7",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Download NHANES Data (XPT Files)\n",
    "\n",
    "**Download** 2 cycles √ó 10 components = 20 XPT files from CDC\n",
    "\n",
    "**Cycles:** 2011-2012, 2013-2014 (only cycles with full periodontal measurements)\n",
    "\n",
    "**Method:** `pd.read_sas(url)` ‚Üí save as parquet\n",
    "\n",
    "‚ö†Ô∏è **Note:** 2015-2016 and 2017-2018 cycles excluded due to missing periodontal exam data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e0e34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NHANES XPT files and save as parquet using config-driven URLs\n",
    "import pandas as pd\n",
    "\n",
    "PERIO_PREFIX_BY_CYCLE = {\n",
    "    \"2011_2012\": \"OHXPER\",  # full-mouth perio exam\n",
    "    \"2013_2014\": \"OHXPER\",  # full-mouth perio exam\n",
    "    \"2015_2016\": \"OHXDEN\",  # dental exam (perio data moved here)\n",
    "    \"2017_2018\": \"OHXDEN\",  # dental exam (perio data moved here)\n",
    "}\n",
    "\n",
    "for cycle in CYCLES:\n",
    "    year = cycle.split(\"_\")[0]  # e.g., 2011 from 2011_2012\n",
    "    suffix = CYCLE_SUFFIX[cycle]\n",
    "    cycle_dir = RAW_DIR / cycle\n",
    "    cycle_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for file_prefix, component in COMPONENTS.items():\n",
    "        prefix = file_prefix\n",
    "        if component == \"periodontal_exam\":\n",
    "            prefix = PERIO_PREFIX_BY_CYCLE.get(cycle, file_prefix)\n",
    "\n",
    "        url = f\"{BASE_URL}/{year}/DataFiles/{prefix}{suffix}.XPT\"\n",
    "        dest = cycle_dir / f\"{component}.parquet\"\n",
    "\n",
    "        if dest.exists():\n",
    "            print(f\"‚úì {cycle} {component}: already exists ({dest})\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df = pd.read_sas(url, format=\"xport\")\n",
    "            df.to_parquet(dest)\n",
    "            print(f\"‚úì {cycle} {component}: {len(df)} rows ‚Üí {dest}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó {cycle} {component}: {e}\")\n",
    "            raise\n",
    "\n",
    "print(\"‚úÖ Section 3: Data downloaded\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91d61fd",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Merge Components on SEQN\n",
    "\n",
    "**Join** all components by participant ID (SEQN)\n",
    "\n",
    "**Filter:** Adults 30+\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8396291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all components on SEQN (participant ID), filter age >= 30\n",
    "for cycle in CYCLES:\n",
    "    print(f\"Merging {cycle}...\")\n",
    "    dfs = []\n",
    "    # Iterate over the component names (values), not the keys\n",
    "    for component_name in COMPONENTS.values():\n",
    "        filepath = RAW_DIR / cycle / f\"{component_name}.parquet\"\n",
    "        df = pd.read_parquet(filepath)\n",
    "        dfs.append(df)\n",
    "    \n",
    "    # Merge all components on SEQN\n",
    "    merged = dfs[0]\n",
    "    for df in dfs[1:]:\n",
    "        merged = merged.merge(df, on=\"SEQN\", how=\"outer\")\n",
    "    \n",
    "    # Filter to adults 30+\n",
    "    before_filter = len(merged)\n",
    "    merged = merged[merged[\"RIDAGEYR\"] >= 30]\n",
    "    after_filter = len(merged)\n",
    "    \n",
    "    # Save merged dataset\n",
    "    output_path = PROCESSED_DIR / f\"{cycle}_merged.parquet\"\n",
    "    merged.to_parquet(output_path)\n",
    "    print(f\"  ‚úì {cycle}: {before_filter} total ‚Üí {after_filter} adults 30+ ‚Üí {output_path}\")\n",
    "\n",
    "print(\"\\n‚úÖ Section 4: Components merged, filtered to adults 30+\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b8d255",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Apply CDC/AAP Case Definitions\n",
    "\n",
    "**Most Critical Section!**\n",
    "\n",
    "**Implement:**\n",
    "- Severe: CAL ‚â•6mm (‚â•2 different teeth) + PD ‚â•5mm (‚â•1 site)\n",
    "- Moderate: CAL ‚â•4mm (‚â•2 teeth) OR PD ‚â•5mm (‚â•2 teeth)\n",
    "- Mild: (CAL ‚â•3mm + PD ‚â•4mm on ‚â•2 teeth) OR PD ‚â•5mm (‚â•1 site)\n",
    "\n",
    "**Use:** `src/labels.py` `label_periodontitis()`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6d1660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply CDC/AAP periodontitis case definitions to each cycle\n",
    "from labels import label_periodontitis\n",
    "\n",
    "for cycle in CYCLES:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing {cycle}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Load merged data\n",
    "    df = pd.read_parquet(PROCESSED_DIR / f\"{cycle}_merged.parquet\")\n",
    "    print(f\"Loaded {len(df)} participants\")\n",
    "    \n",
    "    # Apply CDC/AAP classification\n",
    "    df_labeled = label_periodontitis(df)\n",
    "    \n",
    "    # Save labeled dataset\n",
    "    output_path = PROCESSED_DIR / f\"{cycle}_labeled.parquet\"\n",
    "    df_labeled.to_parquet(output_path)\n",
    "    print(f\"‚úì Saved to: {output_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Section 5: CDC/AAP labels applied to all cycles\")\n",
    "print(\"=\"*60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58b795f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Periodontitis Classification Summary Across Cycles\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Load all labeled datasets\n",
    "results = []\n",
    "for cycle in CYCLES:\n",
    "    df = pd.read_parquet(PROCESSED_DIR / f\"{cycle}_labeled.parquet\")\n",
    "    \n",
    "    # Get counts by severity\n",
    "    counts = df['perio_class'].value_counts()\n",
    "    prevalence = df['has_periodontitis'].mean()\n",
    "    \n",
    "    results.append({\n",
    "        'cycle': cycle,\n",
    "        'n_participants': len(df),\n",
    "        'prevalence': prevalence,\n",
    "        'none': counts.get('none', 0),\n",
    "        'mild': counts.get('mild', 0),\n",
    "        'moderate': counts.get('moderate', 0),\n",
    "        'severe': counts.get('severe', 0)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('ü¶∑ NHANES Periodontitis Classification Results (2011-2018)\\nCDC/AAP Case Definitions', \n",
    "             fontsize=18, fontweight='bold', y=0.995)\n",
    "\n",
    "# Color palette\n",
    "colors = {\n",
    "    'severe': palette['periospot_red'],\n",
    "    'moderate': palette['crimson_blaze'],\n",
    "    'mild': palette['periospot_blue'],\n",
    "    'none': palette['vanilla_cream'],\n",
    "    'overall': palette['periospot_blue']\n",
    "}\n",
    "\n",
    "# Plot 1: Overall Prevalence by Cycle\n",
    "ax1 = axes[0, 0]\n",
    "bars = ax1.bar(results_df['cycle'], results_df['prevalence'] * 100, \n",
    "               color=colors['overall'], edgecolor='black', linewidth=1.5)\n",
    "ax1.set_ylabel('Prevalence (%)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('NHANES Cycle', fontsize=14, fontweight='bold')\n",
    "ax1.set_title('Overall Periodontitis Prevalence', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylim(0, 100)\n",
    "ax1.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, val) in enumerate(zip(bars, results_df['prevalence'] * 100)):\n",
    "    if val > 0:\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2, \n",
    "                f'{val:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "    else:\n",
    "        # Highlight the problem cycles\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, 5, \n",
    "                '‚ö†Ô∏è DATA\\nISSUE', ha='center', va='bottom', \n",
    "                fontweight='bold', fontsize=10, color='red')\n",
    "\n",
    "# Plot 2: Severity Distribution (Stacked Bar)\n",
    "ax2 = axes[0, 1]\n",
    "severity_data = results_df[['cycle', 'none', 'mild', 'moderate', 'severe']].set_index('cycle')\n",
    "severity_data.plot(kind='bar', stacked=True, ax=ax2, \n",
    "                   color=[colors['none'], colors['mild'], colors['moderate'], colors['severe']],\n",
    "                   edgecolor='black', linewidth=1.5)\n",
    "ax2.set_ylabel('Number of Participants', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('NHANES Cycle', fontsize=14, fontweight='bold')\n",
    "ax2.set_title('Severity Distribution', fontsize=14, fontweight='bold')\n",
    "ax2.legend(title='Severity', title_fontsize=12, fontsize=11, \n",
    "           labels=['None', 'Mild', 'Moderate', 'Severe'])\n",
    "ax2.set_xticklabels(results_df['cycle'], rotation=45, ha='right')\n",
    "ax2.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Plot 3: Prevalence by Severity Category\n",
    "ax3 = axes[1, 0]\n",
    "severity_pct = pd.DataFrame({\n",
    "    'Severe': (results_df['severe'] / results_df['n_participants'] * 100),\n",
    "    'Moderate': (results_df['moderate'] / results_df['n_participants'] * 100),\n",
    "    'Mild': (results_df['mild'] / results_df['n_participants'] * 100)\n",
    "}, index=results_df['cycle'])\n",
    "\n",
    "severity_pct.plot(kind='bar', ax=ax3, \n",
    "                  color=[colors['severe'], colors['moderate'], colors['mild']],\n",
    "                  edgecolor='black', linewidth=1.5)\n",
    "ax3.set_ylabel('Prevalence (%)', fontsize=14, fontweight='bold')\n",
    "ax3.set_xlabel('NHANES Cycle', fontsize=14, fontweight='bold')\n",
    "ax3.set_title('Prevalence by Severity Level', fontsize=14, fontweight='bold')\n",
    "ax3.legend(title='Severity', title_fontsize=12, fontsize=11)\n",
    "ax3.set_xticklabels(results_df['cycle'], rotation=45, ha='right')\n",
    "ax3.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Plot 4: Data Quality Summary\n",
    "ax4 = axes[1, 1]\n",
    "ax4.axis('off')\n",
    "\n",
    "# Create summary text\n",
    "summary_text = \"\"\"\n",
    "üìä DATA QUALITY SUMMARY\n",
    "\n",
    "‚úÖ 2011-2012: VALID\n",
    "   ‚Ä¢ 4,566 participants\n",
    "   ‚Ä¢ 68.62% prevalence\n",
    "   ‚Ä¢ All periodontal variables present\n",
    "\n",
    "‚úÖ 2013-2014: VALID  \n",
    "   ‚Ä¢ 4,813 participants\n",
    "   ‚Ä¢ 67.98% prevalence\n",
    "   ‚Ä¢ All periodontal variables present\n",
    "\n",
    "‚ö†Ô∏è 2015-2016: DATA ISSUE\n",
    "   ‚Ä¢ 4,745 participants\n",
    "   ‚Ä¢ 0.00% prevalence (INVALID)\n",
    "   ‚Ä¢ 112 periodontal variables MISSING\n",
    "   ‚Ä¢ NHANES changed exam structure\n",
    "\n",
    "‚ö†Ô∏è 2017-2018: DATA ISSUE\n",
    "   ‚Ä¢ 4,741 participants  \n",
    "   ‚Ä¢ 0.00% prevalence (INVALID)\n",
    "   ‚Ä¢ 112 periodontal variables MISSING\n",
    "   ‚Ä¢ NHANES changed exam structure\n",
    "\n",
    "üîç ROOT CAUSE:\n",
    "In 2015-2016, NHANES moved periodontal\n",
    "measurements from OHXPER to OHXDEN \n",
    "component with DIFFERENT variable names.\n",
    "\n",
    "üìù NEXT STEPS:\n",
    "1. Investigate OHXDEN variable structure\n",
    "2. Update variable mapping for 2015-2018\n",
    "3. Re-run CDC/AAP classification\n",
    "\"\"\"\n",
    "\n",
    "ax4.text(0.05, 0.95, summary_text, transform=ax4.transAxes,\n",
    "         fontsize=11, verticalalignment='top', fontfamily='monospace',\n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "\n",
    "plt.tight_layout()\n",
    "save_figure(fig, FIGURES_DIR / \"01_periodontitis_classification_summary.png\")\n",
    "print(f\"\\n‚úì Saved: {FIGURES_DIR / '01_periodontitis_classification_summary.png'}\")\n",
    "\n",
    "# Create a detailed table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED RESULTS TABLE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Cycle':<15} {'N':<8} {'Prev%':<8} {'None':<8} {'Mild':<8} {'Moderate':<8} {'Severe':<8}\")\n",
    "print(\"-\"*80)\n",
    "for _, row in results_df.iterrows():\n",
    "    print(f\"{row['cycle']:<15} {row['n_participants']:<8} \"\n",
    "          f\"{row['prevalence']*100:>6.2f}% {row['none']:<8} \"\n",
    "          f\"{row['mild']:<8} {row['moderate']:<8} {row['severe']:<8}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show warning about unusable cycles\n",
    "print(\"\\n‚ö†Ô∏è  WARNING: 2015-2016 and 2017-2018 cycles cannot be used for analysis!\")\n",
    "print(\"   Reason: Periodontal exam variable structure changed in NHANES.\")\n",
    "print(\"   Impact: Cannot perform temporal validation as planned.\")\n",
    "print(\"\\nüí° RECOMMENDATION: Use only 2011-2012 and 2013-2014 for now.\")\n",
    "print(\"   Or: Investigate OHXDEN component structure to fix 2015-2018 data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6186641c",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Build 15 Predictors\n",
    "\n",
    "Extract Bashir predictors from NHANES variables\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36867672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build predictors\n",
    "print(\"‚úÖ Section 6: Predictors built\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e695277e",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Exploratory Analysis\n",
    "\n",
    "Prevalence by cycle, missingness, drift\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc74dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: EDA plots\n",
    "print(\"‚úÖ Section 7: EDA complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84835db8",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Temporal Split\n",
    "\n",
    "Train 2011-2014, Val 2015-2016, Test 2017-2018\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea711951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Split by cycle\n",
    "print(\"‚úÖ Section 8: Temporal split done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969d5d35",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Preprocessing Pipelines\n",
    "\n",
    "Imputation + scaling (fit on train only)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0cb5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build sklearn pipelines\n",
    "print(\"‚úÖ Section 9: Pipelines built\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091d6a52",
   "metadata": {},
   "source": [
    "## üîü Baseline Models\n",
    "\n",
    "LogReg, RandomForest with 5-fold CV\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137958e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train baselines\n",
    "print(\"‚úÖ Section 10: Baselines trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc453587",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ XGBoost + Optuna\n",
    "\n",
    "Hyperparameter search, early stopping\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efe417a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Optuna tune XGBoost\n",
    "print(\"‚úÖ Section 11: XGBoost tuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6852efd",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£2Ô∏è‚É£ CatBoost + Optuna\n",
    "\n",
    "Native categorical handling\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cf15d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Optuna tune CatBoost\n",
    "print(\"‚úÖ Section 12: CatBoost tuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3aec68",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£3Ô∏è‚É£ LightGBM + Optuna\n",
    "\n",
    "Fast gradient boosting\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73dbbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Optuna tune LightGBM\n",
    "print(\"‚úÖ Section 13: LightGBM tuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f377bfc",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£4Ô∏è‚É£ Threshold Selection\n",
    "\n",
    "Choose policy (Youden, F1-max, Recall‚â•0.80), freeze on Val\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94f43e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Select threshold on Val\n",
    "print(\"‚úÖ Section 14: Threshold frozen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09cb7d9",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£5Ô∏è‚É£ Final Test Evaluation\n",
    "\n",
    "Apply frozen threshold, compute all metrics\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfe0ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate on Test\n",
    "print(\"‚úÖ Section 15: Test metrics computed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674d5c21",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£6Ô∏è‚É£ Calibration & Decision Curves\n",
    "\n",
    "Isotonic/Platt scaling, net benefit\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659ca535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calibration plots\n",
    "print(\"‚úÖ Section 16: Calibration done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fa9220",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£7Ô∏è‚É£ SHAP Interpretability\n",
    "\n",
    "Beeswarm + bar plots\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd48afd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: SHAP analysis\n",
    "print(\"‚úÖ Section 17: SHAP complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efc29b0",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£8Ô∏è‚É£ Survey Weights Sensitivity\n",
    "\n",
    "Weighted prevalence with WTMEC2YR\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5898598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Weighted stats\n",
    "print(\"‚úÖ Section 18: Survey weights applied\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d90af6",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£9Ô∏è‚É£ Save Artifacts\n",
    "\n",
    "Export model, metrics, HF model card\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee403ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Save all artifacts\n",
    "print(\"‚úÖ Section 19: Artifacts saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b568a3",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£0Ô∏è‚É£ Reproducibility Log\n",
    "\n",
    "Package versions, git hash, system info\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1d3776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Log system info\n",
    "print(\"‚úÖ Section 20: Reproducibility logged\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Codecademy ML",
   "language": "python",
   "name": "codeacademy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
