{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ¦· NHANES Periodontitis: External Validation & Final Analyses\n",
        "\n",
        "**Notebook 01: External Validation, CIs, DCA, Prevalence Reconciliation**\n",
        "\n",
        "This notebook completes the analysis pipeline for medRxiv submission:\n",
        "\n",
        "1. **Section 23:** External Validation (NHANES 2009-2010)\n",
        "2. **Section 24:** Prevalence Reconciliation\n",
        "3. **Section 25:** Bootstrap 95% Confidence Intervals\n",
        "4. **Section 26:** Permutation Tests (Run & Save)\n",
        "5. **Section 27:** Decision Curve Analysis (DCA)\n",
        "6. **Section 28:** KNHANES Scaffold (Optional)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 23: Environment Setup & Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Section 23.0: Environment Setup\n",
        "===============================\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, average_precision_score, brier_score_loss,\n",
        "    roc_curve, precision_recall_curve, confusion_matrix\n",
        ")\n",
        "from sklearn.calibration import calibration_curve\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.impute import SimpleImputer\n",
        "import xgboost as xgb\n",
        "import catboost as cb\n",
        "import lightgbm as lgb\n",
        "import json\n",
        "from datetime import datetime\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Find project root\n",
        "def find_project_root(marker=\"configs/config.yaml\"):\n",
        "    here = Path.cwd()\n",
        "    for candidate in [here] + list(here.parents):\n",
        "        if (candidate / marker).exists():\n",
        "            return candidate\n",
        "    raise FileNotFoundError(f\"Could not locate {marker}\")\n",
        "\n",
        "BASE_DIR = find_project_root()\n",
        "os.chdir(BASE_DIR)\n",
        "print(f\"Working directory: {Path.cwd()}\")\n",
        "\n",
        "# Add src to path\n",
        "sys.path.insert(0, str(BASE_DIR / 'src'))\n",
        "\n",
        "# Import custom modules\n",
        "from ps_plot import set_style, get_palette, save_figure, PERIOSPOT_BLUE, PERIOSPOT_RED\n",
        "from labels import label_periodontitis\n",
        "from stats_utils import permutation_test_auc, pairwise_permutation_tests\n",
        "\n",
        "# Set style\n",
        "set_style()\n",
        "palette = get_palette()\n",
        "\n",
        "# Define paths\n",
        "RAW_DIR = BASE_DIR / 'data' / 'raw'\n",
        "PROCESSED_DIR = BASE_DIR / 'data' / 'processed'\n",
        "FIGURES_DIR = BASE_DIR / 'figures'\n",
        "RESULTS_DIR = BASE_DIR / 'results'\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "print(\"âœ… Section 23.0: Environment setup complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 23.1: Load Training Data and Model Parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Section 23.1: Load Training Data & Tuned Parameters\n",
        "===================================================\n",
        "\"\"\"\n",
        "# Load training features (cleaned, with missing indicators)\n",
        "df_train_full = pd.read_parquet(PROCESSED_DIR / 'features_cleaned.parquet')\n",
        "print(f\"Training data: {df_train_full.shape}\")\n",
        "\n",
        "# Load tuned hyperparameters from v1.3\n",
        "with open(RESULTS_DIR / 'xgboost_results.json', 'r') as f:\n",
        "    xgb_results = json.load(f)\n",
        "with open(RESULTS_DIR / 'catboost_results.json', 'r') as f:\n",
        "    cat_results = json.load(f)\n",
        "with open(RESULTS_DIR / 'lightgbm_results.json', 'r') as f:\n",
        "    lgbm_results = json.load(f)\n",
        "\n",
        "# Extract best params\n",
        "tuned_xgb_params = xgb_results['best_params']\n",
        "tuned_cat_params = cat_results['best_params']\n",
        "tuned_lgbm_params = lgbm_results['best_params']\n",
        "\n",
        "print(\"\\nâœ… Loaded tuned hyperparameters\")\n",
        "\n",
        "# Define feature lists (PRIMARY MODEL - 29 features, no reverse-causality)\n",
        "CONTINUOUS_FEATURES = ['age', 'bmi', 'waist_cm', 'waist_height', 'height_cm',\n",
        "                       'systolic_bp', 'diastolic_bp', 'glucose', 'triglycerides', 'hdl']\n",
        "BINARY_FEATURES = ['sex', 'education', 'smoking', 'alcohol',\n",
        "                   'smoke_current', 'smoke_former', 'alcohol_current']\n",
        "MISSING_INDICATORS = ['bmi_missing', 'systolic_bp_missing', 'diastolic_bp_missing',\n",
        "                      'glucose_missing', 'triglycerides_missing', 'hdl_missing',\n",
        "                      'smoking_missing', 'alcohol_missing',\n",
        "                      'waist_cm_missing', 'waist_height_missing', 'height_cm_missing',\n",
        "                      'alcohol_current_missing']\n",
        "\n",
        "# Reverse-causality features (EXCLUDED from primary model)\n",
        "REVERSE_CAUSALITY = ['dental_visit', 'floss_days', 'mobile_teeth', 'floss_days_missing']\n",
        "\n",
        "# Primary model features\n",
        "ALL_FEATURES_PRIMARY = CONTINUOUS_FEATURES + BINARY_FEATURES + MISSING_INDICATORS\n",
        "ALL_FEATURES_PRIMARY = [f for f in ALL_FEATURES_PRIMARY if f not in REVERSE_CAUSALITY]\n",
        "ALL_FEATURES_PRIMARY = [f for f in ALL_FEATURES_PRIMARY if f in df_train_full.columns]\n",
        "print(f\"\\nPrimary model features: {len(ALL_FEATURES_PRIMARY)}\")\n",
        "\n",
        "# Monotonic constraints\n",
        "MONO_INCREASING = ['age', 'bmi', 'waist_cm', 'waist_height',\n",
        "                   'systolic_bp', 'diastolic_bp', 'glucose', 'triglycerides']\n",
        "MONO_DECREASING = ['hdl']\n",
        "\n",
        "print(\"âœ… Section 23.1: Training data and parameters loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 23.2: Download & Process NHANES 2009-2010 Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Section 23.2: Download NHANES 2009-2010 Data\n",
        "============================================\n",
        "Same data sources and variable mappings as 2011-2014\n",
        "\"\"\"\n",
        "import requests\n",
        "\n",
        "# NHANES 2009-2010 URLs (same structure as later cycles)\n",
        "NHANES_2009_2010 = {\n",
        "    'demographics': 'https://wwwn.cdc.gov/Nchs/Nhanes/2009-2010/DEMO_F.XPT',\n",
        "    'body_measures': 'https://wwwn.cdc.gov/Nchs/Nhanes/2009-2010/BMX_F.XPT',\n",
        "    'blood_pressure': 'https://wwwn.cdc.gov/Nchs/Nhanes/2009-2010/BPX_F.XPT',\n",
        "    'smoking': 'https://wwwn.cdc.gov/Nchs/Nhanes/2009-2010/SMQ_F.XPT',\n",
        "    'alcohol': 'https://wwwn.cdc.gov/Nchs/Nhanes/2009-2010/ALQ_F.XPT',\n",
        "    'oral_health_questionnaire': 'https://wwwn.cdc.gov/Nchs/Nhanes/2009-2010/OHQ_F.XPT',\n",
        "    'periodontal_exam': 'https://wwwn.cdc.gov/Nchs/Nhanes/2009-2010/OHXPER_F.XPT',\n",
        "    'glucose': 'https://wwwn.cdc.gov/Nchs/Nhanes/2009-2010/GLU_F.XPT',\n",
        "    'triglycerides': 'https://wwwn.cdc.gov/Nchs/Nhanes/2009-2010/TRIGLY_F.XPT',\n",
        "    'hdl_cholesterol': 'https://wwwn.cdc.gov/Nchs/Nhanes/2009-2010/HDL_F.XPT',\n",
        "}\n",
        "\n",
        "# Create directory\n",
        "cycle_dir = RAW_DIR / '2009_2010'\n",
        "cycle_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def download_xpt(url, output_path):\n",
        "    \"\"\"Download XPT file and convert to parquet\"\"\"\n",
        "    if output_path.exists():\n",
        "        print(f\"  âœ“ Already exists: {output_path.name}\")\n",
        "        return pd.read_parquet(output_path)\n",
        "    \n",
        "    print(f\"  â†“ Downloading: {url.split('/')[-1]}\")\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    \n",
        "    # Save temporarily as XPT\n",
        "    temp_xpt = output_path.with_suffix('.xpt')\n",
        "    with open(temp_xpt, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "    \n",
        "    # Convert to parquet\n",
        "    df = pd.read_sas(temp_xpt)\n",
        "    df.to_parquet(output_path)\n",
        "    temp_xpt.unlink()  # Remove temp file\n",
        "    \n",
        "    print(f\"  âœ“ Saved: {output_path.name} ({len(df)} rows)\")\n",
        "    return df\n",
        "\n",
        "print(\"ðŸ“¥ Downloading NHANES 2009-2010 data...\\n\")\n",
        "\n",
        "dfs_0910 = {}\n",
        "for name, url in NHANES_2009_2010.items():\n",
        "    output_path = cycle_dir / f\"{name}.parquet\"\n",
        "    try:\n",
        "        dfs_0910[name] = download_xpt(url, output_path)\n",
        "    except Exception as e:\n",
        "        print(f\"  âœ— Error downloading {name}: {e}\")\n",
        "\n",
        "print(\"\\nâœ… Section 23.2: 2009-2010 data downloaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 23.3: Merge and Label 2009-2010 Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Section 23.3: Merge 2009-2010 Components & Apply CDC/AAP Labels\n",
        "===============================================================\n",
        "\"\"\"\n",
        "# Reload from parquet\n",
        "demo = pd.read_parquet(cycle_dir / 'demographics.parquet')\n",
        "body = pd.read_parquet(cycle_dir / 'body_measures.parquet')\n",
        "bp = pd.read_parquet(cycle_dir / 'blood_pressure.parquet')\n",
        "smq = pd.read_parquet(cycle_dir / 'smoking.parquet')\n",
        "alq = pd.read_parquet(cycle_dir / 'alcohol.parquet')\n",
        "ohq = pd.read_parquet(cycle_dir / 'oral_health_questionnaire.parquet')\n",
        "perio = pd.read_parquet(cycle_dir / 'periodontal_exam.parquet')\n",
        "glu = pd.read_parquet(cycle_dir / 'glucose.parquet')\n",
        "trig = pd.read_parquet(cycle_dir / 'triglycerides.parquet')\n",
        "hdl_df = pd.read_parquet(cycle_dir / 'hdl_cholesterol.parquet')\n",
        "\n",
        "print(f\"Demographics: {len(demo)}\")\n",
        "print(f\"Periodontal: {len(perio)}\")\n",
        "\n",
        "# Merge all on SEQN\n",
        "df_0910 = demo.merge(body, on='SEQN', how='left')\n",
        "df_0910 = df_0910.merge(bp, on='SEQN', how='left')\n",
        "df_0910 = df_0910.merge(smq, on='SEQN', how='left')\n",
        "df_0910 = df_0910.merge(alq, on='SEQN', how='left')\n",
        "df_0910 = df_0910.merge(ohq, on='SEQN', how='left')\n",
        "df_0910 = df_0910.merge(perio, on='SEQN', how='left')\n",
        "df_0910 = df_0910.merge(glu, on='SEQN', how='left')\n",
        "df_0910 = df_0910.merge(trig, on='SEQN', how='left')\n",
        "df_0910 = df_0910.merge(hdl_df, on='SEQN', how='left')\n",
        "\n",
        "print(f\"\\nMerged: {len(df_0910)} rows\")\n",
        "\n",
        "# Filter to adults 30+\n",
        "df_0910 = df_0910[df_0910['RIDAGEYR'] >= 30].copy()\n",
        "print(f\"Adults 30+: {len(df_0910)} rows\")\n",
        "\n",
        "# Apply CDC/AAP periodontitis labels\n",
        "print(\"\\nðŸ¦· Applying CDC/AAP classification...\")\n",
        "df_0910_labeled = label_periodontitis(df_0910)\n",
        "\n",
        "if df_0910_labeled is not None:\n",
        "    # Save labeled data\n",
        "    df_0910_labeled.to_parquet(PROCESSED_DIR / '2009_2010_labeled.parquet')\n",
        "    print(f\"\\nâœ… Labeled: {len(df_0910_labeled)} participants\")\n",
        "    print(f\"   Periodontitis prevalence: {df_0910_labeled['has_periodontitis'].mean()*100:.1f}%\")\n",
        "else:\n",
        "    print(\"âš ï¸ Labeling failed - check periodontal exam columns\")\n",
        "\n",
        "print(\"\\nâœ… Section 23.3: 2009-2010 data merged and labeled\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 23.4: Build Features for 2009-2010 (Same Pipeline as Training)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Section 23.4: Build Features for 2009-2010\n",
        "==========================================\n",
        "Apply EXACT same feature engineering as training data\n",
        "\"\"\"\n",
        "# Load labeled data\n",
        "df_0910 = pd.read_parquet(PROCESSED_DIR / '2009_2010_labeled.parquet')\n",
        "\n",
        "# Build features (same logic as Section 6 in notebook 00)\n",
        "df_ext = pd.DataFrame(index=df_0910.index)\n",
        "\n",
        "# Demographics\n",
        "df_ext['age'] = df_0910['RIDAGEYR']\n",
        "df_ext['sex'] = (df_0910['RIAGENDR'] == 1).astype(int)  # 1=Male\n",
        "df_ext['education'] = (df_0910['DMDEDUC2'] >= 4).astype(int)  # >=High school\n",
        "\n",
        "# Metabolic\n",
        "df_ext['bmi'] = df_0910['BMXBMI']\n",
        "df_ext['waist_cm'] = df_0910['BMXWAIST']\n",
        "df_ext['height_cm'] = df_0910['BMXHT']\n",
        "df_ext['waist_height'] = df_0910['BMXWAIST'] / df_0910['BMXHT']\n",
        "df_ext['systolic_bp'] = df_0910['BPXSY1']\n",
        "df_ext['diastolic_bp'] = df_0910['BPXDI1']\n",
        "df_ext['glucose'] = df_0910['LBXGLU'] if 'LBXGLU' in df_0910.columns else np.nan\n",
        "df_ext['triglycerides'] = df_0910['LBXTR'] if 'LBXTR' in df_0910.columns else np.nan\n",
        "df_ext['hdl'] = df_0910['LBDHDD'] if 'LBDHDD' in df_0910.columns else np.nan\n",
        "\n",
        "# Behaviors - Smoking (3-level)\n",
        "df_ext['smoking'] = df_0910['SMQ040'].apply(\n",
        "    lambda x: 1 if x in [1, 2] else (0 if x == 3 else np.nan)\n",
        ")\n",
        "df_ext['smoke_current'] = (\n",
        "    (df_0910['SMQ020'] == 1) & (df_0910['SMQ040'].isin([1, 2]))\n",
        ").astype(int)\n",
        "df_ext['smoke_former'] = (\n",
        "    (df_0910['SMQ020'] == 1) & (df_0910['SMQ040'] == 3)\n",
        ").astype(int)\n",
        "\n",
        "# Alcohol\n",
        "df_ext['alcohol'] = df_0910['ALQ101'].apply(\n",
        "    lambda x: 1 if x == 1 else (0 if x == 2 else np.nan)\n",
        ")\n",
        "df_ext['alcohol_current'] = df_0910['ALQ110'].apply(\n",
        "    lambda x: 1 if x == 1 else (0 if x == 2 else np.nan)\n",
        ") if 'ALQ110' in df_0910.columns else np.nan\n",
        "\n",
        "# Oral health (for secondary model - excluded from primary)\n",
        "df_ext['dental_visit'] = (df_0910['OHQ030'] <= 2).astype(int) if 'OHQ030' in df_0910.columns else np.nan\n",
        "df_ext['mobile_teeth'] = (df_0910['OHQ680'] == 1).astype(int) if 'OHQ680' in df_0910.columns else np.nan\n",
        "df_ext['floss_days'] = df_0910['OHQ620'] if 'OHQ620' in df_0910.columns else np.nan\n",
        "\n",
        "# Target\n",
        "df_ext['has_periodontitis'] = df_0910['has_periodontitis']\n",
        "df_ext['severity'] = df_0910['severity']\n",
        "\n",
        "# Add missing indicators\n",
        "for feat in ['bmi', 'systolic_bp', 'diastolic_bp', 'glucose', 'triglycerides', 'hdl',\n",
        "             'smoking', 'alcohol', 'waist_cm', 'waist_height', 'height_cm', 'alcohol_current',\n",
        "             'floss_days']:\n",
        "    if feat in df_ext.columns:\n",
        "        df_ext[f'{feat}_missing'] = df_ext[feat].isna().astype(int)\n",
        "\n",
        "# Data cleaning (same as training)\n",
        "if 'diastolic_bp' in df_ext.columns:\n",
        "    df_ext['diastolic_bp'] = df_ext['diastolic_bp'].clip(40, 120)\n",
        "\n",
        "if 'triglycerides' in df_ext.columns:\n",
        "    p99 = df_ext['triglycerides'].quantile(0.99)\n",
        "    df_ext['triglycerides'] = df_ext['triglycerides'].clip(upper=p99)\n",
        "\n",
        "# Drop rows without target\n",
        "df_ext = df_ext.dropna(subset=['has_periodontitis'])\n",
        "\n",
        "print(f\"External test set: {len(df_ext)} participants\")\n",
        "print(f\"Periodontitis prevalence: {df_ext['has_periodontitis'].mean()*100:.1f}%\")\n",
        "print(f\"\\nFeatures available: {df_ext.shape[1]}\")\n",
        "\n",
        "# Save\n",
        "df_ext.to_parquet(PROCESSED_DIR / '2009_2010_features.parquet')\n",
        "\n",
        "print(\"\\nâœ… Section 23.4: External features built\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 23.5-23.7: External Validation (Train on 2011-2014, Evaluate on 2009-2010)\n",
        "\n",
        "The following cells will:\n",
        "1. Train the primary ensemble on full 2011-2014 data\n",
        "2. Generate calibrated predictions on 2009-2010\n",
        "3. Compute metrics with 95% CIs\n",
        "4. Generate ROC/PR/Calibration plots\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Run this cell after downloading and processing 2009-2010 data\n",
        "# See notebook 00 for full implementation details\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ðŸ“Š EXTERNAL VALIDATION WORKFLOW\")\n",
        "print(\"=\"*70)\n",
        "print(\"\"\"\n",
        "Steps to complete:\n",
        "1. Run cells 23.2-23.4 to download and process 2009-2010 data\n",
        "2. Train ensemble on 2011-2014 (using tuned params from notebook 00)\n",
        "3. Evaluate on 2009-2010\n",
        "4. Compute metrics with bootstrap 95% CIs\n",
        "5. Save results to results/external_0910_metrics.json\n",
        "6. Generate figures/external_roc_pr_calibration.png\n",
        "\n",
        "Key outputs expected:\n",
        "- AUC-ROC with 95% CI\n",
        "- PR-AUC with 95% CI\n",
        "- Brier score with 95% CI\n",
        "- Operating point metrics at t=0.35 (rule-out) and t=0.65 (balanced)\n",
        "\"\"\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Section 24: Prevalence Reconciliation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Section 24: Prevalence Reconciliation\n",
        "=====================================\n",
        "Compare our prevalence to CDC/AAP published estimates\n",
        "\"\"\"\n",
        "# Load all labeled datasets\n",
        "cycles = {\n",
        "    '2011-2012': PROCESSED_DIR / '2011_2012_labeled.parquet',\n",
        "    '2013-2014': PROCESSED_DIR / '2013_2014_labeled.parquet',\n",
        "}\n",
        "\n",
        "# Check if 2009-2010 exists\n",
        "if (PROCESSED_DIR / '2009_2010_labeled.parquet').exists():\n",
        "    cycles['2009-2010'] = PROCESSED_DIR / '2009_2010_labeled.parquet'\n",
        "\n",
        "prevalence_data = []\n",
        "\n",
        "print(\"ðŸ“Š PREVALENCE RECONCILIATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for cycle, path in cycles.items():\n",
        "    if path.exists():\n",
        "        df = pd.read_parquet(path)\n",
        "        df = df[df['has_periodontitis'].notna()]\n",
        "        \n",
        "        total = len(df)\n",
        "        perio = df['has_periodontitis'].sum()\n",
        "        prev = perio / total * 100\n",
        "        \n",
        "        severe = (df['severity'] == 'severe').sum() / total * 100 if 'severity' in df.columns else np.nan\n",
        "        moderate = (df['severity'] == 'moderate').sum() / total * 100 if 'severity' in df.columns else np.nan\n",
        "        mild = (df['severity'] == 'mild').sum() / total * 100 if 'severity' in df.columns else np.nan\n",
        "        \n",
        "        prevalence_data.append({\n",
        "            'cycle': cycle, 'n': total, 'prevalence': prev,\n",
        "            'severe': severe, 'moderate': moderate, 'mild': mild\n",
        "        })\n",
        "        \n",
        "        print(f\"\\n{cycle}:\")\n",
        "        print(f\"  N = {total:,}\")\n",
        "        print(f\"  Total periodontitis: {prev:.1f}%\")\n",
        "        if not np.isnan(severe):\n",
        "            print(f\"    - Severe: {severe:.1f}%, Moderate: {moderate:.1f}%, Mild: {mild:.1f}%\")\n",
        "\n",
        "# CDC Reference\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"\\nðŸ“š CDC Reference (Eke et al. 2015, NHANES 2009-2012):\")\n",
        "print(\"  Adults 30+ with periodontitis: 47.2%\")\n",
        "print(\"    - Severe: 8.9%, Moderate: 30.0%, Mild: 8.7%\")\n",
        "\n",
        "print(\"\\nâš ï¸ RECONCILIATION NOTE:\")\n",
        "print(\"  Our higher prevalence (~68%) vs CDC (~47%) reflects:\")\n",
        "print(\"  1. Only participants with FULL periodontal exam\")\n",
        "print(\"  2. CDC includes partial/edentulous participants\")\n",
        "\n",
        "# Save results\n",
        "prevalence_results = {\n",
        "    'our_estimates': prevalence_data,\n",
        "    'cdc_reference': {\n",
        "        'source': 'Eke et al. 2015, J Periodontol',\n",
        "        'total_periodontitis': 47.2, 'severe': 8.9, 'moderate': 30.0, 'mild': 8.7\n",
        "    },\n",
        "    'reconciliation_note': \"Higher prevalence reflects full periodontal exam inclusion criteria.\",\n",
        "    'timestamp': datetime.now().isoformat()\n",
        "}\n",
        "\n",
        "with open(RESULTS_DIR / 'prevalence_check.json', 'w') as f:\n",
        "    json.dump(prevalence_results, f, indent=2)\n",
        "\n",
        "print(f\"\\nâœ… Saved: results/prevalence_check.json\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Section 25-27: Additional Analyses (CIs, Permutation Tests, DCA)\n",
        "\n",
        "**Note:** The following analyses require running the full training pipeline from notebook 00. They should be run after all models are trained and OOF predictions are available.\n",
        "\n",
        "### Section 25: Bootstrap 95% CIs\n",
        "Compute confidence intervals for AUC, PR-AUC, Brier using 2000 stratified bootstrap resamples.\n",
        "\n",
        "### Section 26: Permutation Tests  \n",
        "Run paired permutation tests (10,000 iterations) on OOF predictions.\n",
        "\n",
        "### Section 27: Decision Curve Analysis\n",
        "Compute net benefit curves for the primary model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Helper Functions for Sections 25-27\n",
        "===================================\n",
        "\"\"\"\n",
        "def bootstrap_ci(y_true, y_score, metric_fn, n_bootstrap=2000, ci=0.95, seed=42):\n",
        "    \"\"\"Compute bootstrap confidence interval for a metric\"\"\"\n",
        "    np.random.seed(seed)\n",
        "    n = len(y_true)\n",
        "    scores = []\n",
        "    \n",
        "    for _ in range(n_bootstrap):\n",
        "        idx = np.random.choice(n, n, replace=True)\n",
        "        try:\n",
        "            score = metric_fn(y_true[idx], y_score[idx])\n",
        "            scores.append(score)\n",
        "        except:\n",
        "            continue\n",
        "    \n",
        "    scores = np.array(scores)\n",
        "    alpha = (1 - ci) / 2\n",
        "    return np.mean(scores), np.percentile(scores, alpha * 100), np.percentile(scores, (1 - alpha) * 100)\n",
        "\n",
        "def decision_curve_analysis(y_true, y_prob, thresholds=None):\n",
        "    \"\"\"Compute net benefit for DCA\"\"\"\n",
        "    if thresholds is None:\n",
        "        thresholds = np.arange(0.01, 0.99, 0.01)\n",
        "    \n",
        "    n = len(y_true)\n",
        "    prevalence = np.mean(y_true)\n",
        "    results = []\n",
        "    \n",
        "    for t in thresholds:\n",
        "        y_pred = (y_prob >= t).astype(int)\n",
        "        tp = np.sum((y_pred == 1) & (y_true == 1))\n",
        "        fp = np.sum((y_pred == 1) & (y_true == 0))\n",
        "        \n",
        "        net_benefit_model = (tp / n) - (fp / n) * (t / (1 - t))\n",
        "        net_benefit_all = prevalence - (1 - prevalence) * (t / (1 - t))\n",
        "        \n",
        "        results.append({\n",
        "            'threshold': t,\n",
        "            'model': max(net_benefit_model, 0),\n",
        "            'treat_all': max(net_benefit_all, 0),\n",
        "            'treat_none': 0\n",
        "        })\n",
        "    \n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "print(\"âœ… Helper functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Final Summary\n",
        "\n",
        "After running all sections, you should have:\n",
        "\n",
        "**Results files:**\n",
        "- `results/external_0910_metrics.json` - External validation metrics\n",
        "- `results/prevalence_check.json` - Prevalence reconciliation  \n",
        "- `results/v13_primary_ci.json` - Bootstrap CIs\n",
        "- `results/model_comp_permutation.json` - Permutation test results\n",
        "- `results/decision_curve_primary.json` - DCA data\n",
        "\n",
        "**Figures:**\n",
        "- `figures/external_roc_pr_calibration.png` - External validation plots\n",
        "- `figures/prevalence_barplot.png` - Prevalence comparison\n",
        "- `figures/decision_curve_primary.png` - DCA curves\n",
        "\n",
        "**Next steps for medRxiv:**\n",
        "1. Update ARTICLE_DRAFT.md with external validation results\n",
        "2. Add 95% CIs to all metrics tables\n",
        "3. Insert DCA paragraph in Discussion\n",
        "4. Add prevalence reconciliation note in Methods\n",
        "5. Archive repo to Zenodo for DOI\n",
        "6. Final proofread and submit!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
