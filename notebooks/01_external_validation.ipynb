{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ü¶∑ NHANES Periodontitis: External Validation & Final Analyses\n",
        "\n",
        "**Notebook 01: External Validation, CIs, DCA, Prevalence Reconciliation**\n",
        "\n",
        "This notebook completes the analysis pipeline for medRxiv submission:\n",
        "\n",
        "1. **Section 23:** External Validation (NHANES 2009-2010)\n",
        "2. **Section 24:** Prevalence Reconciliation\n",
        "3. **Section 25:** Bootstrap 95% Confidence Intervals\n",
        "4. **Section 26:** Permutation Tests (Run & Save)\n",
        "5. **Section 27:** Decision Curve Analysis (DCA)\n",
        "6. **Section 28:** KNHANES Scaffold (Optional)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 23: Environment Setup & Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Working directory: /Users/franciscoteixeirabarbosa/Dropbox/Random_scripts/nhanes_periodontitis_ml\n",
            "‚úÖ Section 23.0: Environment setup complete\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Section 23.0: Environment Setup\n",
        "===============================\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, average_precision_score, brier_score_loss,\n",
        "    roc_curve, precision_recall_curve, confusion_matrix\n",
        ")\n",
        "from sklearn.calibration import calibration_curve\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.impute import SimpleImputer\n",
        "import xgboost as xgb\n",
        "import catboost as cb\n",
        "import lightgbm as lgb\n",
        "import json\n",
        "from datetime import datetime\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Find project root\n",
        "def find_project_root(marker=\"configs/config.yaml\"):\n",
        "    here = Path.cwd()\n",
        "    for candidate in [here] + list(here.parents):\n",
        "        if (candidate / marker).exists():\n",
        "            return candidate\n",
        "    raise FileNotFoundError(f\"Could not locate {marker}\")\n",
        "\n",
        "BASE_DIR = find_project_root()\n",
        "os.chdir(BASE_DIR)\n",
        "print(f\"Working directory: {Path.cwd()}\")\n",
        "\n",
        "# Add src to path\n",
        "sys.path.insert(0, str(BASE_DIR / 'src'))\n",
        "\n",
        "# Import custom modules\n",
        "from ps_plot import set_style, get_palette, save_figure, PERIOSPOT_BLUE, PERIOSPOT_RED\n",
        "from labels import label_periodontitis\n",
        "from stats_utils import permutation_test_auc, pairwise_permutation_tests\n",
        "\n",
        "# Set style\n",
        "set_style()\n",
        "palette = get_palette()\n",
        "\n",
        "# Define paths\n",
        "RAW_DIR = BASE_DIR / 'data' / 'raw'\n",
        "PROCESSED_DIR = BASE_DIR / 'data' / 'processed'\n",
        "FIGURES_DIR = BASE_DIR / 'figures'\n",
        "RESULTS_DIR = BASE_DIR / 'results'\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "print(\"‚úÖ Section 23.0: Environment setup complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 23.1: Load Training Data and Model Parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data: (9379, 37)\n",
            "\n",
            "‚úÖ Loaded tuned hyperparameters\n",
            "\n",
            "Primary model features: 29\n",
            "‚úÖ Section 23.1: Training data and parameters loaded\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Section 23.1: Load Training Data & Tuned Parameters\n",
        "===================================================\n",
        "\"\"\"\n",
        "# Load training features (cleaned, with missing indicators)\n",
        "df_train_full = pd.read_parquet(PROCESSED_DIR / 'features_cleaned.parquet')\n",
        "print(f\"Training data: {df_train_full.shape}\")\n",
        "\n",
        "# Load tuned hyperparameters from v1.3\n",
        "with open(RESULTS_DIR / 'xgboost_results.json', 'r') as f:\n",
        "    xgb_results = json.load(f)\n",
        "with open(RESULTS_DIR / 'catboost_results.json', 'r') as f:\n",
        "    cat_results = json.load(f)\n",
        "with open(RESULTS_DIR / 'lightgbm_results.json', 'r') as f:\n",
        "    lgbm_results = json.load(f)\n",
        "\n",
        "# Extract best params\n",
        "tuned_xgb_params = xgb_results['best_params']\n",
        "tuned_cat_params = cat_results['best_params']\n",
        "tuned_lgbm_params = lgbm_results['best_params']\n",
        "\n",
        "print(\"\\n‚úÖ Loaded tuned hyperparameters\")\n",
        "\n",
        "# Define feature lists (PRIMARY MODEL - 29 features, no reverse-causality)\n",
        "CONTINUOUS_FEATURES = ['age', 'bmi', 'waist_cm', 'waist_height', 'height_cm',\n",
        "                       'systolic_bp', 'diastolic_bp', 'glucose', 'triglycerides', 'hdl']\n",
        "BINARY_FEATURES = ['sex', 'education', 'smoking', 'alcohol',\n",
        "                   'smoke_current', 'smoke_former', 'alcohol_current']\n",
        "MISSING_INDICATORS = ['bmi_missing', 'systolic_bp_missing', 'diastolic_bp_missing',\n",
        "                      'glucose_missing', 'triglycerides_missing', 'hdl_missing',\n",
        "                      'smoking_missing', 'alcohol_missing',\n",
        "                      'waist_cm_missing', 'waist_height_missing', 'height_cm_missing',\n",
        "                      'alcohol_current_missing']\n",
        "\n",
        "# Reverse-causality features (EXCLUDED from primary model)\n",
        "REVERSE_CAUSALITY = ['dental_visit', 'floss_days', 'mobile_teeth', 'floss_days_missing']\n",
        "\n",
        "# Primary model features\n",
        "ALL_FEATURES_PRIMARY = CONTINUOUS_FEATURES + BINARY_FEATURES + MISSING_INDICATORS\n",
        "ALL_FEATURES_PRIMARY = [f for f in ALL_FEATURES_PRIMARY if f not in REVERSE_CAUSALITY]\n",
        "ALL_FEATURES_PRIMARY = [f for f in ALL_FEATURES_PRIMARY if f in df_train_full.columns]\n",
        "print(f\"\\nPrimary model features: {len(ALL_FEATURES_PRIMARY)}\")\n",
        "\n",
        "# Monotonic constraints\n",
        "MONO_INCREASING = ['age', 'bmi', 'waist_cm', 'waist_height',\n",
        "                   'systolic_bp', 'diastolic_bp', 'glucose', 'triglycerides']\n",
        "MONO_DECREASING = ['hdl']\n",
        "\n",
        "print(\"‚úÖ Section 23.1: Training data and parameters loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 23.2: Download & Process NHANES 2009-2010 Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Downloading/Converting NHANES 2009-2010 data...\n",
            "\n",
            "  ‚Üì Downloading: DEMO_F.XPT\n",
            "  ‚ö†Ô∏è Warning: File seems small (20905 bytes)\n",
            "  ‚úó Error with demographics: Header record is not an XPORT file.\n",
            "  ‚Üì Downloading: BMX_F.XPT\n",
            "  ‚ö†Ô∏è Warning: File seems small (20905 bytes)\n",
            "  ‚úó Error with body_measures: Header record is not an XPORT file.\n",
            "  ‚Üì Downloading: BPX_F.XPT\n",
            "  ‚ö†Ô∏è Warning: File seems small (20905 bytes)\n",
            "  ‚úó Error with blood_pressure: Header record is not an XPORT file.\n",
            "  ‚Üì Downloading: SMQ_F.XPT\n",
            "  ‚ö†Ô∏è Warning: File seems small (20905 bytes)\n",
            "  ‚úó Error with smoking: Header record is not an XPORT file.\n",
            "  ‚Üì Downloading: ALQ_F.XPT\n",
            "  ‚ö†Ô∏è Warning: File seems small (20905 bytes)\n",
            "  ‚úó Error with alcohol: Header record is not an XPORT file.\n",
            "  ‚Üì Downloading: OHQ_F.XPT\n",
            "  ‚ö†Ô∏è Warning: File seems small (20905 bytes)\n",
            "  ‚úó Error with oral_health_questionnaire: Header record is not an XPORT file.\n",
            "  ‚Üì Downloading: OHXPER_F.XPT\n",
            "  ‚ö†Ô∏è Warning: File seems small (20905 bytes)\n",
            "  ‚úó Error with periodontal_exam: Header record is not an XPORT file.\n",
            "  ‚Üì Downloading: GLU_F.XPT\n",
            "  ‚ö†Ô∏è Warning: File seems small (20905 bytes)\n",
            "  ‚úó Error with glucose: Header record is not an XPORT file.\n",
            "  ‚Üì Downloading: TRIGLY_F.XPT\n",
            "  ‚ö†Ô∏è Warning: File seems small (20905 bytes)\n",
            "  ‚úó Error with triglycerides: Header record is not an XPORT file.\n",
            "  ‚Üì Downloading: HDL_F.XPT\n",
            "  ‚ö†Ô∏è Warning: File seems small (20905 bytes)\n",
            "  ‚úó Error with hdl_cholesterol: Header record is not an XPORT file.\n",
            "\n",
            "‚úÖ Section 23.2: 2009-2010 data downloaded/converted\n",
            "\n",
            "üìä Data summary:\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Section 23.2: Download NHANES 2009-2010 Data\n",
        "============================================\n",
        "Same data sources and variable mappings as 2011-2014\n",
        "\"\"\"\n",
        "import requests\n",
        "\n",
        "# NHANES 2009-2010 URLs (UPDATED: New CDC URL structure as of 2024)\n",
        "# Old format: /Nchs/Nhanes/2009-2010/DEMO_F.XPT\n",
        "# New format: /Nchs/Data/Nhanes/Public/2009/DataFiles/DEMO_F.xpt\n",
        "BASE_URL_2009 = 'https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2009/DataFiles'\n",
        "\n",
        "NHANES_2009_2010 = {\n",
        "    'demographics': f'{BASE_URL_2009}/DEMO_F.xpt',\n",
        "    'body_measures': f'{BASE_URL_2009}/BMX_F.xpt',\n",
        "    'blood_pressure': f'{BASE_URL_2009}/BPX_F.xpt',\n",
        "    'smoking': f'{BASE_URL_2009}/SMQ_F.xpt',\n",
        "    'alcohol': f'{BASE_URL_2009}/ALQ_F.xpt',\n",
        "    'oral_health_questionnaire': f'{BASE_URL_2009}/OHQ_F.xpt',\n",
        "    'periodontal_exam': f'{BASE_URL_2009}/OHXPER_F.xpt',\n",
        "    'glucose': f'{BASE_URL_2009}/GLU_F.xpt',\n",
        "    'triglycerides': f'{BASE_URL_2009}/TRIGLY_F.xpt',\n",
        "    'hdl_cholesterol': f'{BASE_URL_2009}/HDL_F.xpt',\n",
        "}\n",
        "\n",
        "# Create directory\n",
        "cycle_dir = RAW_DIR / '2009_2010'\n",
        "cycle_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def download_and_convert_xpt(url, output_path, force_redownload=False):\n",
        "    \"\"\"Download XPT file and convert to parquet\"\"\"\n",
        "    # Check if parquet already exists and is valid\n",
        "    if output_path.exists() and not force_redownload:\n",
        "        try:\n",
        "            df = pd.read_parquet(output_path)\n",
        "            if len(df) > 0:\n",
        "                print(f\"  ‚úì Already exists: {output_path.name} ({len(df)} rows)\")\n",
        "                return df\n",
        "        except:\n",
        "            pass  # File is corrupt, re-download\n",
        "    \n",
        "    # Check if XPT exists and convert\n",
        "    xpt_path = output_path.with_suffix('.xpt')\n",
        "    if xpt_path.exists():\n",
        "        try:\n",
        "            df = pd.read_sas(xpt_path)\n",
        "            if len(df) > 0:\n",
        "                df.to_parquet(output_path)\n",
        "                print(f\"  ‚úì Converted from XPT: {output_path.name} ({len(df)} rows)\")\n",
        "                return df\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö†Ô∏è XPT file corrupt, re-downloading: {e}\")\n",
        "    \n",
        "    # Download fresh\n",
        "    print(f\"  ‚Üì Downloading: {url.split('/')[-1]}\")\n",
        "    response = requests.get(url, timeout=60)\n",
        "    response.raise_for_status()\n",
        "    \n",
        "    # Check file size\n",
        "    if len(response.content) < 50000:  # Less than 50KB is suspicious\n",
        "        print(f\"  ‚ö†Ô∏è Warning: File seems small ({len(response.content)} bytes)\")\n",
        "    \n",
        "    # Save as XPT first\n",
        "    with open(xpt_path, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "    \n",
        "    # Convert to parquet\n",
        "    df = pd.read_sas(xpt_path)\n",
        "    df.to_parquet(output_path)\n",
        "    \n",
        "    print(f\"  ‚úì Downloaded and converted: {output_path.name} ({len(df)} rows)\")\n",
        "    return df\n",
        "\n",
        "print(\"üì• Downloading/Converting NHANES 2009-2010 data...\\n\")\n",
        "\n",
        "dfs_0910 = {}\n",
        "for name, url in NHANES_2009_2010.items():\n",
        "    output_path = cycle_dir / f\"{name}.parquet\"\n",
        "    try:\n",
        "        dfs_0910[name] = download_and_convert_xpt(url, output_path)\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚úó Error with {name}: {e}\")\n",
        "\n",
        "print(\"\\n‚úÖ Section 23.2: 2009-2010 data downloaded/converted\")\n",
        "print(f\"\\nüìä Data summary:\")\n",
        "for name, df in dfs_0910.items():\n",
        "    print(f\"   {name}: {len(df)} rows\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 23.3: Merge and Label 2009-2010 Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/Users/franciscoteixeirabarbosa/Dropbox/Random_scripts/nhanes_periodontitis_ml/data/raw/2009_2010/demographics.parquet'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mSection 23.3: Merge 2009-2010 Components & Apply CDC/AAP Labels\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m===============================================================\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Reload from parquet\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m demo = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcycle_dir\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdemographics.parquet\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m body = pd.read_parquet(cycle_dir / \u001b[33m'\u001b[39m\u001b[33mbody_measures.parquet\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      8\u001b[39m bp = pd.read_parquet(cycle_dir / \u001b[33m'\u001b[39m\u001b[33mblood_pressure.parquet\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/pandas/io/parquet.py:669\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    666\u001b[39m     use_nullable_dtypes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    667\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/pandas/io/parquet.py:258\u001b[39m, in \u001b[36mPyArrowImpl.read\u001b[39m\u001b[34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m manager == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    257\u001b[39m     to_pandas_kwargs[\u001b[33m\"\u001b[39m\u001b[33msplit_blocks\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m path_or_handle, handles, filesystem = \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    265\u001b[39m     pa_table = \u001b[38;5;28mself\u001b[39m.api.parquet.read_table(\n\u001b[32m    266\u001b[39m         path_or_handle,\n\u001b[32m    267\u001b[39m         columns=columns,\n\u001b[32m   (...)\u001b[39m\u001b[32m    270\u001b[39m         **kwargs,\n\u001b[32m    271\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/pandas/io/parquet.py:141\u001b[39m, in \u001b[36m_get_path_or_handle\u001b[39m\u001b[34m(path, fs, storage_options, mode, is_dir)\u001b[39m\n\u001b[32m    131\u001b[39m handles = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    133\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[32m    134\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[32m   (...)\u001b[39m\u001b[32m    139\u001b[39m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m     fs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    145\u001b[39m     path_or_handle = handles.handle\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/pandas/io/common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/Users/franciscoteixeirabarbosa/Dropbox/Random_scripts/nhanes_periodontitis_ml/data/raw/2009_2010/demographics.parquet'"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Section 23.3: Merge 2009-2010 Components & Apply CDC/AAP Labels\n",
        "===============================================================\n",
        "\"\"\"\n",
        "# Reload from parquet\n",
        "demo = pd.read_parquet(cycle_dir / 'demographics.parquet')\n",
        "body = pd.read_parquet(cycle_dir / 'body_measures.parquet')\n",
        "bp = pd.read_parquet(cycle_dir / 'blood_pressure.parquet')\n",
        "smq = pd.read_parquet(cycle_dir / 'smoking.parquet')\n",
        "alq = pd.read_parquet(cycle_dir / 'alcohol.parquet')\n",
        "ohq = pd.read_parquet(cycle_dir / 'oral_health_questionnaire.parquet')\n",
        "perio = pd.read_parquet(cycle_dir / 'periodontal_exam.parquet')\n",
        "glu = pd.read_parquet(cycle_dir / 'glucose.parquet')\n",
        "trig = pd.read_parquet(cycle_dir / 'triglycerides.parquet')\n",
        "hdl_df = pd.read_parquet(cycle_dir / 'hdl_cholesterol.parquet')\n",
        "\n",
        "print(f\"Demographics: {len(demo)}\")\n",
        "print(f\"Periodontal: {len(perio)}\")\n",
        "\n",
        "# Merge all on SEQN\n",
        "df_0910 = demo.merge(body, on='SEQN', how='left')\n",
        "df_0910 = df_0910.merge(bp, on='SEQN', how='left')\n",
        "df_0910 = df_0910.merge(smq, on='SEQN', how='left')\n",
        "df_0910 = df_0910.merge(alq, on='SEQN', how='left')\n",
        "df_0910 = df_0910.merge(ohq, on='SEQN', how='left')\n",
        "df_0910 = df_0910.merge(perio, on='SEQN', how='left')\n",
        "df_0910 = df_0910.merge(glu, on='SEQN', how='left')\n",
        "df_0910 = df_0910.merge(trig, on='SEQN', how='left')\n",
        "df_0910 = df_0910.merge(hdl_df, on='SEQN', how='left')\n",
        "\n",
        "print(f\"\\nMerged: {len(df_0910)} rows\")\n",
        "\n",
        "# Filter to adults 30+\n",
        "df_0910 = df_0910[df_0910['RIDAGEYR'] >= 30].copy()\n",
        "print(f\"Adults 30+: {len(df_0910)} rows\")\n",
        "\n",
        "# Apply CDC/AAP periodontitis labels\n",
        "print(\"\\nü¶∑ Applying CDC/AAP classification...\")\n",
        "df_0910_labeled = label_periodontitis(df_0910)\n",
        "\n",
        "if df_0910_labeled is not None:\n",
        "    # Save labeled data\n",
        "    df_0910_labeled.to_parquet(PROCESSED_DIR / '2009_2010_labeled.parquet')\n",
        "    print(f\"\\n‚úÖ Labeled: {len(df_0910_labeled)} participants\")\n",
        "    print(f\"   Periodontitis prevalence: {df_0910_labeled['has_periodontitis'].mean()*100:.1f}%\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Labeling failed - check periodontal exam columns\")\n",
        "\n",
        "print(\"\\n‚úÖ Section 23.3: 2009-2010 data merged and labeled\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 23.4: Build Features for 2009-2010 (Same Pipeline as Training)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Section 23.4: Build Features for 2009-2010\n",
        "==========================================\n",
        "Apply EXACT same feature engineering as training data\n",
        "\"\"\"\n",
        "# Load labeled data\n",
        "df_0910 = pd.read_parquet(PROCESSED_DIR / '2009_2010_labeled.parquet')\n",
        "\n",
        "# Build features (same logic as Section 6 in notebook 00)\n",
        "df_ext = pd.DataFrame(index=df_0910.index)\n",
        "\n",
        "# Demographics\n",
        "df_ext['age'] = df_0910['RIDAGEYR']\n",
        "df_ext['sex'] = (df_0910['RIAGENDR'] == 1).astype(int)  # 1=Male\n",
        "df_ext['education'] = (df_0910['DMDEDUC2'] >= 4).astype(int)  # >=High school\n",
        "\n",
        "# Metabolic\n",
        "df_ext['bmi'] = df_0910['BMXBMI']\n",
        "df_ext['waist_cm'] = df_0910['BMXWAIST']\n",
        "df_ext['height_cm'] = df_0910['BMXHT']\n",
        "df_ext['waist_height'] = df_0910['BMXWAIST'] / df_0910['BMXHT']\n",
        "df_ext['systolic_bp'] = df_0910['BPXSY1']\n",
        "df_ext['diastolic_bp'] = df_0910['BPXDI1']\n",
        "df_ext['glucose'] = df_0910['LBXGLU'] if 'LBXGLU' in df_0910.columns else np.nan\n",
        "df_ext['triglycerides'] = df_0910['LBXTR'] if 'LBXTR' in df_0910.columns else np.nan\n",
        "df_ext['hdl'] = df_0910['LBDHDD'] if 'LBDHDD' in df_0910.columns else np.nan\n",
        "\n",
        "# Behaviors - Smoking (3-level)\n",
        "df_ext['smoking'] = df_0910['SMQ040'].apply(\n",
        "    lambda x: 1 if x in [1, 2] else (0 if x == 3 else np.nan)\n",
        ")\n",
        "df_ext['smoke_current'] = (\n",
        "    (df_0910['SMQ020'] == 1) & (df_0910['SMQ040'].isin([1, 2]))\n",
        ").astype(int)\n",
        "df_ext['smoke_former'] = (\n",
        "    (df_0910['SMQ020'] == 1) & (df_0910['SMQ040'] == 3)\n",
        ").astype(int)\n",
        "\n",
        "# Alcohol\n",
        "df_ext['alcohol'] = df_0910['ALQ101'].apply(\n",
        "    lambda x: 1 if x == 1 else (0 if x == 2 else np.nan)\n",
        ")\n",
        "df_ext['alcohol_current'] = df_0910['ALQ110'].apply(\n",
        "    lambda x: 1 if x == 1 else (0 if x == 2 else np.nan)\n",
        ") if 'ALQ110' in df_0910.columns else np.nan\n",
        "\n",
        "# Oral health (for secondary model - excluded from primary)\n",
        "df_ext['dental_visit'] = (df_0910['OHQ030'] <= 2).astype(int) if 'OHQ030' in df_0910.columns else np.nan\n",
        "df_ext['mobile_teeth'] = (df_0910['OHQ680'] == 1).astype(int) if 'OHQ680' in df_0910.columns else np.nan\n",
        "df_ext['floss_days'] = df_0910['OHQ620'] if 'OHQ620' in df_0910.columns else np.nan\n",
        "\n",
        "# Target\n",
        "df_ext['has_periodontitis'] = df_0910['has_periodontitis']\n",
        "df_ext['severity'] = df_0910['severity']\n",
        "\n",
        "# Add missing indicators\n",
        "for feat in ['bmi', 'systolic_bp', 'diastolic_bp', 'glucose', 'triglycerides', 'hdl',\n",
        "             'smoking', 'alcohol', 'waist_cm', 'waist_height', 'height_cm', 'alcohol_current',\n",
        "             'floss_days']:\n",
        "    if feat in df_ext.columns:\n",
        "        df_ext[f'{feat}_missing'] = df_ext[feat].isna().astype(int)\n",
        "\n",
        "# Data cleaning (same as training)\n",
        "if 'diastolic_bp' in df_ext.columns:\n",
        "    df_ext['diastolic_bp'] = df_ext['diastolic_bp'].clip(40, 120)\n",
        "\n",
        "if 'triglycerides' in df_ext.columns:\n",
        "    p99 = df_ext['triglycerides'].quantile(0.99)\n",
        "    df_ext['triglycerides'] = df_ext['triglycerides'].clip(upper=p99)\n",
        "\n",
        "# Drop rows without target\n",
        "df_ext = df_ext.dropna(subset=['has_periodontitis'])\n",
        "\n",
        "print(f\"External test set: {len(df_ext)} participants\")\n",
        "print(f\"Periodontitis prevalence: {df_ext['has_periodontitis'].mean()*100:.1f}%\")\n",
        "print(f\"\\nFeatures available: {df_ext.shape[1]}\")\n",
        "\n",
        "# Save\n",
        "df_ext.to_parquet(PROCESSED_DIR / '2009_2010_features.parquet')\n",
        "\n",
        "print(\"\\n‚úÖ Section 23.4: External features built\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 23.5-23.7: External Validation (Train on 2011-2014, Evaluate on 2009-2010)\n",
        "\n",
        "The following cells will:\n",
        "1. Train the primary ensemble on full 2011-2014 data\n",
        "2. Generate calibrated predictions on 2009-2010\n",
        "3. Compute metrics with 95% CIs\n",
        "4. Generate ROC/PR/Calibration plots\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Run this cell after downloading and processing 2009-2010 data\n",
        "# See notebook 00 for full implementation details\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"üìä EXTERNAL VALIDATION WORKFLOW\")\n",
        "print(\"=\"*70)\n",
        "print(\"\"\"\n",
        "Steps to complete:\n",
        "1. Run cells 23.2-23.4 to download and process 2009-2010 data\n",
        "2. Train ensemble on 2011-2014 (using tuned params from notebook 00)\n",
        "3. Evaluate on 2009-2010\n",
        "4. Compute metrics with bootstrap 95% CIs\n",
        "5. Save results to results/external_0910_metrics.json\n",
        "6. Generate figures/external_roc_pr_calibration.png\n",
        "\n",
        "Key outputs expected:\n",
        "- AUC-ROC with 95% CI\n",
        "- PR-AUC with 95% CI\n",
        "- Brier score with 95% CI\n",
        "- Operating point metrics at t=0.35 (rule-out) and t=0.65 (balanced)\n",
        "\"\"\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Section 24: Prevalence Reconciliation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Section 24: Prevalence Reconciliation\n",
        "=====================================\n",
        "Compare our prevalence to CDC/AAP published estimates\n",
        "\"\"\"\n",
        "# Load all labeled datasets\n",
        "cycles = {\n",
        "    '2011-2012': PROCESSED_DIR / '2011_2012_labeled.parquet',\n",
        "    '2013-2014': PROCESSED_DIR / '2013_2014_labeled.parquet',\n",
        "}\n",
        "\n",
        "# Check if 2009-2010 exists\n",
        "if (PROCESSED_DIR / '2009_2010_labeled.parquet').exists():\n",
        "    cycles['2009-2010'] = PROCESSED_DIR / '2009_2010_labeled.parquet'\n",
        "\n",
        "prevalence_data = []\n",
        "\n",
        "print(\"üìä PREVALENCE RECONCILIATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for cycle, path in cycles.items():\n",
        "    if path.exists():\n",
        "        df = pd.read_parquet(path)\n",
        "        df = df[df['has_periodontitis'].notna()]\n",
        "        \n",
        "        total = len(df)\n",
        "        perio = df['has_periodontitis'].sum()\n",
        "        prev = perio / total * 100\n",
        "        \n",
        "        severe = (df['severity'] == 'severe').sum() / total * 100 if 'severity' in df.columns else np.nan\n",
        "        moderate = (df['severity'] == 'moderate').sum() / total * 100 if 'severity' in df.columns else np.nan\n",
        "        mild = (df['severity'] == 'mild').sum() / total * 100 if 'severity' in df.columns else np.nan\n",
        "        \n",
        "        prevalence_data.append({\n",
        "            'cycle': cycle, 'n': total, 'prevalence': prev,\n",
        "            'severe': severe, 'moderate': moderate, 'mild': mild\n",
        "        })\n",
        "        \n",
        "        print(f\"\\n{cycle}:\")\n",
        "        print(f\"  N = {total:,}\")\n",
        "        print(f\"  Total periodontitis: {prev:.1f}%\")\n",
        "        if not np.isnan(severe):\n",
        "            print(f\"    - Severe: {severe:.1f}%, Moderate: {moderate:.1f}%, Mild: {mild:.1f}%\")\n",
        "\n",
        "# CDC Reference\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"\\nüìö CDC Reference (Eke et al. 2015, NHANES 2009-2012):\")\n",
        "print(\"  Adults 30+ with periodontitis: 47.2%\")\n",
        "print(\"    - Severe: 8.9%, Moderate: 30.0%, Mild: 8.7%\")\n",
        "\n",
        "print(\"\\n‚ö†Ô∏è RECONCILIATION NOTE:\")\n",
        "print(\"  Our higher prevalence (~68%) vs CDC (~47%) reflects:\")\n",
        "print(\"  1. Only participants with FULL periodontal exam\")\n",
        "print(\"  2. CDC includes partial/edentulous participants\")\n",
        "\n",
        "# Save results\n",
        "prevalence_results = {\n",
        "    'our_estimates': prevalence_data,\n",
        "    'cdc_reference': {\n",
        "        'source': 'Eke et al. 2015, J Periodontol',\n",
        "        'total_periodontitis': 47.2, 'severe': 8.9, 'moderate': 30.0, 'mild': 8.7\n",
        "    },\n",
        "    'reconciliation_note': \"Higher prevalence reflects full periodontal exam inclusion criteria.\",\n",
        "    'timestamp': datetime.now().isoformat()\n",
        "}\n",
        "\n",
        "with open(RESULTS_DIR / 'prevalence_check.json', 'w') as f:\n",
        "    json.dump(prevalence_results, f, indent=2)\n",
        "\n",
        "print(f\"\\n‚úÖ Saved: results/prevalence_check.json\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Section 25-27: Additional Analyses (CIs, Permutation Tests, DCA)\n",
        "\n",
        "**Note:** The following analyses require running the full training pipeline from notebook 00. They should be run after all models are trained and OOF predictions are available.\n",
        "\n",
        "### Section 25: Bootstrap 95% CIs\n",
        "Compute confidence intervals for AUC, PR-AUC, Brier using 2000 stratified bootstrap resamples.\n",
        "\n",
        "### Section 26: Permutation Tests  \n",
        "Run paired permutation tests (10,000 iterations) on OOF predictions.\n",
        "\n",
        "### Section 27: Decision Curve Analysis\n",
        "Compute net benefit curves for the primary model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Helper Functions for Sections 25-27\n",
        "===================================\n",
        "\"\"\"\n",
        "def bootstrap_ci(y_true, y_score, metric_fn, n_bootstrap=2000, ci=0.95, seed=42):\n",
        "    \"\"\"Compute bootstrap confidence interval for a metric\"\"\"\n",
        "    np.random.seed(seed)\n",
        "    n = len(y_true)\n",
        "    scores = []\n",
        "    \n",
        "    for _ in range(n_bootstrap):\n",
        "        idx = np.random.choice(n, n, replace=True)\n",
        "        try:\n",
        "            score = metric_fn(y_true[idx], y_score[idx])\n",
        "            scores.append(score)\n",
        "        except:\n",
        "            continue\n",
        "    \n",
        "    scores = np.array(scores)\n",
        "    alpha = (1 - ci) / 2\n",
        "    return np.mean(scores), np.percentile(scores, alpha * 100), np.percentile(scores, (1 - alpha) * 100)\n",
        "\n",
        "def decision_curve_analysis(y_true, y_prob, thresholds=None):\n",
        "    \"\"\"Compute net benefit for DCA\"\"\"\n",
        "    if thresholds is None:\n",
        "        thresholds = np.arange(0.01, 0.99, 0.01)\n",
        "    \n",
        "    n = len(y_true)\n",
        "    prevalence = np.mean(y_true)\n",
        "    results = []\n",
        "    \n",
        "    for t in thresholds:\n",
        "        y_pred = (y_prob >= t).astype(int)\n",
        "        tp = np.sum((y_pred == 1) & (y_true == 1))\n",
        "        fp = np.sum((y_pred == 1) & (y_true == 0))\n",
        "        \n",
        "        net_benefit_model = (tp / n) - (fp / n) * (t / (1 - t))\n",
        "        net_benefit_all = prevalence - (1 - prevalence) * (t / (1 - t))\n",
        "        \n",
        "        results.append({\n",
        "            'threshold': t,\n",
        "            'model': max(net_benefit_model, 0),\n",
        "            'treat_all': max(net_benefit_all, 0),\n",
        "            'treat_none': 0\n",
        "        })\n",
        "    \n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "print(\"‚úÖ Helper functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Final Summary\n",
        "\n",
        "After running all sections, you should have:\n",
        "\n",
        "**Results files:**\n",
        "- `results/external_0910_metrics.json` - External validation metrics\n",
        "- `results/prevalence_check.json` - Prevalence reconciliation  \n",
        "- `results/v13_primary_ci.json` - Bootstrap CIs\n",
        "- `results/model_comp_permutation.json` - Permutation test results\n",
        "- `results/decision_curve_primary.json` - DCA data\n",
        "\n",
        "**Figures:**\n",
        "- `figures/external_roc_pr_calibration.png` - External validation plots\n",
        "- `figures/prevalence_barplot.png` - Prevalence comparison\n",
        "- `figures/decision_curve_primary.png` - DCA curves\n",
        "\n",
        "**Next steps for medRxiv:**\n",
        "1. Update ARTICLE_DRAFT.md with external validation results\n",
        "2. Add 95% CIs to all metrics tables\n",
        "3. Insert DCA paragraph in Discussion\n",
        "4. Add prevalence reconciliation note in Methods\n",
        "5. Archive repo to Zenodo for DOI\n",
        "6. Final proofread and submit!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Codecademy ML",
      "language": "python",
      "name": "codeacademy"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
